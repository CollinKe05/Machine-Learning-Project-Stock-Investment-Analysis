import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import random
import os
import json
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf

plt.rcParams['font.sans-serif'] = ['SimSun']
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.unicode_minus'] = False


# ================= 1. è®¾ç½®å…¨å±€éšæœºç§å­ =================
def set_random_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰ç›¸å…³éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    # Pythonå’Œç³»ç»Ÿç¯å¢ƒ
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

    # Pythonéšæœºæ¨¡å—
    random.seed(seed)
    np.random.seed(seed)

    # TensorFlow/Keras
    tf.random.set_seed(seed)
    tf.keras.utils.set_random_seed(seed)

    # å°è¯•å¯ç”¨ç¡®å®šæ€§æ“ä½œ
    try:
        tf.config.experimental.enable_op_determinism()
    except:
        pass  # æ—§ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒ

    # é…ç½®GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(f"GPUé…ç½®é”™è¯¯: {e}")


# ================= 2. æ•°æ®åˆ’åˆ†å‚æ•° =================
FILE_PATH = '00700_cleaned.csv'
PRE_TRAIN_END = '2023-06-30'  # é¢„è®­ç»ƒé›†ç»“æŸ
VALID_START = '2023-07-01'  # éªŒè¯é›†å¼€å§‹
VALID_END = '2023-12-31'  # éªŒè¯é›†ç»“æŸ
TEST_START = '2024-01-01'  # æµ‹è¯•é›†å¼€å§‹
LOOKBACK = 15
EPOCHS = 60
BATCH_SIZE = 16
COMMISSION = 0
SEED = 42  # å›ºå®šéšæœºç§å­


# ================= 3. æ•°æ®åŠ è½½ä¸åˆ’åˆ†å‡½æ•° =================
def calculate_macd(df, fast=12, slow=26, signal=9):
    exp1 = df['Close'].ewm(span=fast, adjust=False).mean()
    exp2 = df['Close'].ewm(span=slow, adjust=False).mean()
    df['DIF'] = exp1 - exp2
    df['DEA'] = df['DIF'].ewm(span=signal, adjust=False).mean()
    df['MACD_Hist'] = (df['DIF'] - df['DEA']) * 2
    return df


def process_stock_data(file_path):
    df = pd.read_csv(file_path)
    df['Date'] = pd.to_datetime(df['Date'])
    df.sort_values('Date', inplace=True)
    df.reset_index(drop=True, inplace=True)
    df['MA5'] = df['Close'].rolling(5).mean()
    df['MA20'] = df['Close'].rolling(20).mean()
    df = calculate_macd(df)
    df['Momentum'] = df['Close'] - df['Close'].shift(5)
    df.dropna(inplace=True)
    df.reset_index(drop=True, inplace=True)
    return df


def create_dataset(dataset_X, dataset_Y, look_back=1):
    X, Y = [], []
    for i in range(len(dataset_X) - look_back):
        X.append(dataset_X[i:(i + look_back)])
        Y.append(dataset_Y[i + look_back])
    return np.array(X), np.array(Y)


# ================= 4. æ”¹è¿›çš„è®­ç»ƒç­–ç•¥ =================
def train_with_validation():
    """ä½¿ç”¨éªŒè¯é›†é€‰æ‹©æœ€ä½³æ¨¡å‹"""
    # è®¾ç½®å…¨å±€éšæœºç§å­
    set_random_seeds(SEED)

    print(">>> è¯»å–å’Œå¤„ç†æ•°æ®...")
    df = process_stock_data(FILE_PATH)

    # æ•°æ®åˆ’åˆ†ï¼šä¸‰é˜¶æ®µ
    pre_train_df = df[df['Date'] <= PRE_TRAIN_END].copy()
    valid_df = df[(df['Date'] >= VALID_START) & (df['Date'] <= VALID_END)].copy()
    test_df_raw = df[df['Date'] >= TEST_START].copy()

    print(
        f"é¢„è®­ç»ƒé›†: {len(pre_train_df)} å¤© ({pre_train_df['Date'].min().date()} - {pre_train_df['Date'].max().date()})")
    print(f"éªŒè¯é›†: {len(valid_df)} å¤© ({valid_df['Date'].min().date()} - {valid_df['Date'].max().date()})")
    print(f"æµ‹è¯•é›†: {len(test_df_raw)} å¤© ({test_df_raw['Date'].min().date()} - {test_df_raw['Date'].max().date()})")

    # ç‰¹å¾åˆ—
    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA5', 'MA20',
                    'DIF', 'DEA', 'MACD_Hist']
    target_col = ['Close']

    # åˆ›å»ºscalerï¼ˆä»…åœ¨é¢„è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰
    scaler_X = MinMaxScaler(feature_range=(0, 1))
    scaler_Y = MinMaxScaler(feature_range=(0, 1))

    scaler_X.fit(pre_train_df[feature_cols])
    scaler_Y.fit(pre_train_df[target_col])

    # å‡†å¤‡é¢„è®­ç»ƒæ•°æ®
    pre_train_X_scaled = scaler_X.transform(pre_train_df[feature_cols])
    pre_train_Y_scaled = scaler_Y.transform(pre_train_df[target_col])
    pre_train_X, pre_train_Y = create_dataset(pre_train_X_scaled, pre_train_Y_scaled, LOOKBACK)

    # å‡†å¤‡éªŒè¯æ•°æ®
    full_valid = pd.concat((pre_train_df.iloc[-LOOKBACK:], valid_df))
    valid_X_scaled_long = scaler_X.transform(full_valid[feature_cols])
    valid_X, _ = create_dataset(valid_X_scaled_long, np.zeros(len(valid_X_scaled_long)), LOOKBACK)

    # ================= 5. è®­ç»ƒé˜¶æ®µ =================
    print("\n>>> å¼€å§‹è®­ç»ƒï¼ˆ10ä¸ªå‚æ•°ç»„åˆï¼‰...")

    # å°è¯•ä¸åŒçš„è¶…å‚æ•°
    best_model = None
    best_valid_return = -float('inf')
    best_params = {}

    # è®°å½•æ‰€æœ‰å‚æ•°ç»„åˆç»“æœ
    results_history = []

    # ================= 10ä¸ªç²¾é€‰å‚æ•°ç»„åˆ =================
    # ç²¾å¿ƒæŒ‘é€‰çš„10ä¸ªç»„åˆï¼Œè¦†ç›–ä¸åŒé…ç½®
    parameter_combinations = [
        # (units1, units2, dropout, learning_rate, description)
        (128, 64, 0.2, 0.001, "æ ‡å‡†ä¸­å‹ç½‘ç»œ"),
        (64, 32, 0.3, 0.001, "å°å‹ä¿å®ˆç½‘ç»œ"),
        (256, 128, 0.2, 0.0005, "å¤§å‹ç½‘ç»œä½å­¦ä¹ ç‡"),
        (128, 128, 0.3, 0.001, "å¯¹ç§°ç½‘ç»œ"),
        (64, 64, 0.4, 0.001, "ç´§å‡‘é«˜Dropout"),
        (128, 64, 0.3, 0.0005, "ä¸­å‹ä¿å®ˆ"),
        (256, 256, 0.2, 0.001, "å¤§å‹å¯¹ç§°"),
        (64, 32, 0.2, 0.0005, "å°å‹ä½å­¦ä¹ ç‡"),
        (128, 128, 0.4, 0.0005, "å¯¹ç§°é«˜Dropout"),
        (256, 128, 0.3, 0.001, "å¤§å‹æ ‡å‡†"),
    ]

    total_combinations = len(parameter_combinations)

    for idx, (units1, units2, dropout_rate, lr, description) in enumerate(parameter_combinations, 1):
        print(f"\n--- å°è¯•ç»„åˆ {idx}/{total_combinations}: {description} ---")
        print(f"LSTM: ({units1}, {units2}), Dropout: {dropout_rate}, LR: {lr}")

        # æ¸…é™¤ä¹‹å‰çš„è®¡ç®—å›¾å¹¶é‡æ–°è®¾ç½®ç§å­
        tf.keras.backend.clear_session()
        set_random_seeds(SEED + idx)  # å¾®è°ƒç§å­

        # æ„å»ºæ¨¡å‹
        model = Sequential()
        model.add(LSTM(units1, return_sequences=True,
                       input_shape=(LOOKBACK, len(feature_cols))))
        model.add(Dropout(dropout_rate))
        model.add(LSTM(units2, return_sequences=False))
        model.add(Dropout(dropout_rate))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(1))

        # ä½¿ç”¨è‡ªå®šä¹‰å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨
        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
        model.compile(loss='mse', optimizer=optimizer)

        # æ—©åœï¼ˆç›‘æ§è®­ç»ƒæŸå¤±ï¼‰
        early_stop = EarlyStopping(
            monitor='loss',
            patience=6,
            restore_best_weights=True
        )

        # è®­ç»ƒ
        history = model.fit(
            pre_train_X, pre_train_Y,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=0,
            callbacks=[early_stop]
        )

        # ================= 6. éªŒè¯é˜¶æ®µ =================
        print(">>> åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°...")

        # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
        valid_predict = model.predict(valid_X, verbose=0)
        valid_predict_real = scaler_Y.inverse_transform(valid_predict)

        # åˆ›å»ºéªŒè¯ç»“æœDataFrame
        valid_len = min(len(valid_df), len(valid_predict_real))
        valid_result_df = valid_df.iloc[:valid_len].copy()
        valid_result_df['Predicted'] = valid_predict_real[:valid_len].flatten()

        # åœ¨éªŒè¯é›†ä¸Šæ‰§è¡Œç­–ç•¥
        valid_return = run_strategy_on_data(valid_result_df)

        # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡æŒ‡æ ‡
        y_true_val = valid_result_df['Close'].values
        y_pred_val = valid_result_df['Predicted'].values
        r2_val = r2_score(y_true_val, y_pred_val)
        rmse_val = math.sqrt(mean_squared_error(y_true_val, y_pred_val))

        print(f"éªŒè¯é›†æ”¶ç›Šç‡: {valid_return * 100:.2f}%")
        print(f"éªŒè¯é›†R2: {r2_val:.4f}, RMSE: {rmse_val:.2f}")

        # ä¿å­˜ç»“æœå†å²
        result_info = {
            'ç»„åˆ': idx,
            'æè¿°': description,
            'units1': units1,
            'units2': units2,
            'dropout': dropout_rate,
            'learning_rate': lr,
            'valid_return': valid_return,
            'r2_score': r2_val,
            'rmse': rmse_val,
            'epochs_trained': len(history.history['loss'])
        }
        results_history.append(result_info)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_return > best_valid_return:
            best_valid_return = valid_return
            best_model = model
            best_params = {
                'ç»„åˆ': idx,
                'æè¿°': description,
                'units': (units1, units2),
                'dropout': dropout_rate,
                'learning_rate': lr,
                'valid_return': valid_return,
                'r2_score': r2_val,
                'rmse': rmse_val
            }
            print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯æ”¶ç›Šç‡: {valid_return * 100:.2f}%")

    # ================= 7. ç»“æœåˆ†æ =================
    print(f"\n{'=' * 60}")
    print("å‚æ•°æœç´¢å®Œæˆï¼ç»“æœåˆ†æ:")
    print(f"{'=' * 60}")

    # æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
    results_df = pd.DataFrame(results_history)
    results_df = results_df.sort_values('valid_return', ascending=False)

    print("\nğŸ“Š æ‰€æœ‰å‚æ•°ç»„åˆç»“æœï¼ˆæŒ‰æ”¶ç›Šç‡æ’åºï¼‰:")
    print(results_df[['ç»„åˆ', 'æè¿°', 'valid_return', 'r2_score', 'rmse']].to_string())

    print(f"\n{'=' * 60}")
    print("ğŸ¯ æœ€ä½³æ¨¡å‹å‚æ•°:")
    print(f"ç»„åˆ: {best_params['ç»„åˆ']} - {best_params['æè¿°']}")
    print(f"LSTM Units: {best_params['units']}")
    print(f"Dropout Rate: {best_params['dropout']}")
    print(f"Learning Rate: {best_params['learning_rate']}")
    print(f"éªŒè¯é›†æ”¶ç›Šç‡: {best_params['valid_return'] * 100:.2f}%")
    print(f"éªŒè¯é›†R2 Score: {best_params['r2_score']:.4f}")
    print(f"éªŒè¯é›†RMSE: {best_params['rmse']:.2f}")
    print(f"{'=' * 60}")

    # ä¿å­˜ç»“æœå†å²
    results_df.to_csv('top10_parameter_results.csv', index=False, encoding='utf-8')
    print("âœ… å‚æ•°æœç´¢ç»“æœå·²ä¿å­˜åˆ° top10_parameter_results.csv")

    # ================= 8. æµ‹è¯•é˜¶æ®µï¼ˆæœ€ç»ˆè¯„ä¼°ï¼‰ =================
    print("\n>>> åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    full_test = pd.concat((df[df['Date'] <= VALID_END].iloc[-LOOKBACK:], test_df_raw))
    test_X_scaled_long = scaler_X.transform(full_test[feature_cols])
    test_X, _ = create_dataset(test_X_scaled_long, np.zeros(len(test_X_scaled_long)), LOOKBACK)

    # é¢„æµ‹
    test_predict = best_model.predict(test_X, verbose=0)
    test_predict_real = scaler_Y.inverse_transform(test_predict)

    # åˆ›å»ºæµ‹è¯•ç»“æœ
    valid_len = min(len(test_df_raw), len(test_predict_real))
    result_df = test_df_raw.iloc[:valid_len].copy()
    result_df['Predicted'] = test_predict_real[:valid_len].flatten()

    # è®¡ç®—æµ‹è¯•é›†æ”¶ç›Šç‡
    test_return = run_strategy_on_data(result_df)

    # è¯„ä¼°æŒ‡æ ‡
    y_true = result_df['Close'].values
    y_pred = result_df['Predicted'].values
    r2 = r2_score(y_true, y_pred)
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))

    print(f"\n{'=' * 60}")
    print("æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"R2 Score: {r2:.4f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"æµ‹è¯•é›†æ”¶ç›Šç‡: {test_return * 100:.2f}%")
    print(f"éªŒè¯é›†æ”¶ç›Šç‡: {best_valid_return * 100:.2f}%")
    print(f"{'=' * 60}")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_return > -0.1:  # å…è®¸å°å¹…è´Ÿæ”¶ç›Š
        model_filename = f'best_model_top10_combo{best_params["ç»„åˆ"]}.keras'
        best_model.save(model_filename)
        print(f"\nâœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: {model_filename}")

        # ä¿å­˜å‚æ•°è®°å½•
        params_record = {
            'best_valid_return': float(best_valid_return),
            'test_return': float(test_return),
            'r2_score': float(r2),
            'rmse': float(rmse),
            'best_params': best_params,
            'data_split': {
                'pre_train_end': PRE_TRAIN_END,
                'valid_start': VALID_START,
                'valid_end': VALID_END,
                'test_start': TEST_START
            },
            'random_seed': SEED,
            'feature_cols': feature_cols,
            'lookback': LOOKBACK,
            'epochs': EPOCHS,
            'batch_size': BATCH_SIZE,
            'all_results': results_history
        }

        with open('best_model_params_top10.json', 'w', encoding='utf-8') as f:
            json.dump(params_record, f, indent=2, ensure_ascii=False)

        # å¯è§†åŒ–
        create_final_report(result_df, r2, test_return, best_valid_return, results_df)

    return best_model, result_df, test_return, results_df


# ================= 9. ç­–ç•¥æ‰§è¡Œå‡½æ•° =================
def run_strategy_on_data(result_df):
    """åœ¨ç»™å®šæ•°æ®ä¸Šæ‰§è¡Œäº¤æ˜“ç­–ç•¥"""
    cash = 100000
    position = 0
    assets = []

    # è®¡ç®—è¿è¡Œä¸­çš„MA5
    result_df['Run_MA5'] = result_df['Close'].rolling(5).mean().fillna(method='bfill')

    for i in range(len(result_df) - 1):
        price = result_df.iloc[i]['Close']
        pred_next = result_df.iloc[i]['Predicted']
        ma5 = result_df.iloc[i]['Run_MA5']
        dif = result_df.iloc[i]['DIF']
        dea = result_df.iloc[i]['DEA']
        pred_ret = (pred_next - price) / price

        if position == 0:
            cond1 = price > ma5
            cond2 = dif > dea
            cond3 = pred_ret > 0.01
            if cond1 or cond2 or cond3:
                shares = cash // price
                if shares > 0:
                    cash -= shares * price
                    position = shares

        elif position > 0:
            trend_bad = price < ma5
            macd_bad = dif < dea
            ai_panic = pred_ret < -0.015
            if (trend_bad and macd_bad) or ai_panic:
                cash += position * price
                position = 0

        assets.append(cash + position * price)

    assets.append(cash + position * result_df.iloc[-1]['Close'])
    result_df['Asset'] = assets

    final_return = (assets[-1] - 100000) / 100000
    return final_return


# ================= 10. ç»“æœå¯è§†åŒ– =================
def create_final_report(result_df, r2, test_return, valid_return, results_df):
    """åˆ›å»ºæœ€ç»ˆæŠ¥å‘Šå›¾è¡¨"""
    fig, axes = plt.subplots(3, 2, figsize=(18, 15))

    # å­å›¾1ï¼šä»·æ ¼é¢„æµ‹
    axes[0, 0].plot(result_df['Date'], result_df['Close'], label='çœŸå®è‚¡ä»·', color='blue', linewidth=2)
    axes[0, 0].plot(result_df['Date'], result_df['Predicted'], label='é¢„æµ‹è‚¡ä»·',
                    color='orange', linestyle='--', alpha=0.8)
    axes[0, 0].set_title(f'æµ‹è¯•é›†é¢„æµ‹å¯¹æ¯” | R2: {r2:.4f}', fontsize=14, fontproperties='SimSun')
    axes[0, 0].legend(prop={'family': 'SimSun'}, loc='upper left')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_ylabel('ä»·æ ¼', fontproperties='SimSun')

    # å­å›¾2ï¼šå‚æ•°ç»„åˆæ”¶ç›Šç‡åˆ†å¸ƒ
    returns = results_df['valid_return'] * 100
    axes[0, 1].bar(range(len(returns)), returns, color=['red' if r == max(returns) else 'skyblue' for r in returns])
    axes[0, 1].axhline(y=returns.mean(), color='green', linestyle='--', label=f'å¹³å‡: {returns.mean():.1f}%')
    axes[0, 1].set_title('10ä¸ªå‚æ•°ç»„åˆçš„éªŒè¯é›†æ”¶ç›Šç‡', fontsize=14, fontproperties='SimSun')
    axes[0, 1].set_xlabel('å‚æ•°ç»„åˆç¼–å·', fontproperties='SimSun')
    axes[0, 1].set_ylabel('æ”¶ç›Šç‡ (%)', fontproperties='SimSun')
    axes[0, 1].legend(prop={'family': 'SimSun'})
    axes[0, 1].grid(True, alpha=0.3)

    # å­å›¾3ï¼šç­–ç•¥å‡€å€¼
    benchmark = result_df['Close'] / result_df['Close'].iloc[0]
    strategy = result_df['Asset'] / 100000

    axes[1, 0].plot(result_df['Date'], benchmark,
                    label=f'åŸºå‡†å‡€å€¼ ({benchmark.iloc[-1] * 100 - 100:.1f}%)', color='gray', alpha=0.7)
    axes[1, 0].plot(result_df['Date'], strategy,
                    label=f'ç­–ç•¥å‡€å€¼ ({test_return * 100:.1f}%)', color='red', linewidth=2.5)
    axes[1, 0].set_title(f'æµ‹è¯•é›†è¡¨ç° | ç­–ç•¥æ”¶ç›Š: {test_return * 100:.2f}% (éªŒè¯: {valid_return * 100:.2f}%)',
                         fontsize=14, fontproperties='SimSun')
    axes[1, 0].legend(prop={'family': 'SimSun'}, loc='upper left')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_ylabel('å‡€å€¼', fontproperties='SimSun')

    # å­å›¾4ï¼šå‚æ•°ç»„åˆR2åˆ†å¸ƒ
    r2_scores = results_df['r2_score']
    axes[1, 1].bar(range(len(r2_scores)), r2_scores,
                   color=['red' if r == max(r2_scores) else 'lightgreen' for r in r2_scores])
    axes[1, 1].axhline(y=r2_scores.mean(), color='blue', linestyle='--', label=f'å¹³å‡: {r2_scores.mean():.4f}')
    axes[1, 1].set_title('å‚æ•°ç»„åˆçš„R2åˆ†æ•°åˆ†å¸ƒ', fontsize=14, fontproperties='SimSun')
    axes[1, 1].set_xlabel('å‚æ•°ç»„åˆç¼–å·', fontproperties='SimSun')
    axes[1, 1].set_ylabel('R2åˆ†æ•°', fontproperties='SimSun')
    axes[1, 1].legend(prop={'family': 'SimSun'})
    axes[1, 1].grid(True, alpha=0.3)

    # å­å›¾5ï¼šå›æ’¤åˆ†æ
    asset_series = pd.Series(result_df['Asset'].values)
    cumulative_max = asset_series.cummax()
    drawdown = (asset_series - cumulative_max) / cumulative_max * 100
    axes[2, 0].fill_between(result_df['Date'], drawdown, 0, color='red', alpha=0.3)
    axes[2, 0].axhline(y=0, color='black', linewidth=0.5)
    axes[2, 0].axhline(y=-10, color='orange', linestyle='--', alpha=0.5)
    axes[2, 0].axhline(y=-20, color='red', linestyle='--', alpha=0.5)
    max_dd = drawdown.min()
    axes[2, 0].set_title(f'å›æ’¤åˆ†æ | æœ€å¤§å›æ’¤: {max_dd:.1f}%', fontsize=14, fontproperties='SimSun')
    axes[2, 0].grid(True, alpha=0.3)
    axes[2, 0].set_xlabel('æ—¥æœŸ', fontproperties='SimSun')
    axes[2, 0].set_ylabel('å›æ’¤ (%)', fontproperties='SimSun')

    # å­å›¾6ï¼šæœ€ä½³å‚æ•°ç»„åˆç‰¹å¾
    best_row = results_df.iloc[0]
    features = ['units1', 'units2', 'dropout', 'learning_rate']
    values = [best_row['units1'], best_row['units2'],
              best_row['dropout'], best_row['learning_rate']]
    labels = [f'LSTM1: {values[0]}', f'LSTM2: {values[1]}',
              f'Dropout: {values[2]}', f'LR: {values[3]}']

    axes[2, 1].barh(range(len(features)), values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
    axes[2, 1].set_yticks(range(len(features)))
    axes[2, 1].set_yticklabels(labels, fontproperties='SimSun')
    axes[2, 1].set_title(f'æœ€ä½³ç»„åˆå‚æ•° (æ”¶ç›Šç‡: {best_row["valid_return"] * 100:.1f}%)',
                         fontsize=14, fontproperties='SimSun')
    axes[2, 1].grid(True, alpha=0.3, axis='x')

    plt.tight_layout()
    plt.savefig('Top10_Parameter_Report.png', dpi=150, bbox_inches='tight')
    plt.show()


# ================= 11. ä¸»å‡½æ•° =================
def main():
    # è®¾ç½®å…¨å±€éšæœºç§å­
    set_random_seeds(SEED)

    print("=" * 60)
    print("ç²¾é€‰LSTMæ¨¡å‹è®­ç»ƒä¸éªŒè¯ç³»ç»Ÿ")
    print(f"éšæœºç§å­: {SEED} (ç¡®ä¿ç»“æœå¯é‡å¤)")
    print("æ•°æ®åˆ’åˆ†: é¢„è®­ç»ƒé›† | éªŒè¯é›† | æµ‹è¯•é›†")
    print(f"å‚æ•°æœç´¢: 10ä¸ªç²¾é€‰ç»„åˆ")
    print("=" * 60)

    model, result_df, test_return, results_df = train_with_validation()

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    summary = f"""
    {'=' * 60}
                ç²¾é€‰å‚æ•°ç»„åˆéªŒè¯æŠ¥å‘Š
    {'=' * 60}
    æ•°æ®åˆ’åˆ†:
    - é¢„è®­ç»ƒé›†: æˆªæ­¢ {PRE_TRAIN_END}
    - éªŒè¯é›†: {VALID_START} è‡³ {VALID_END}
    - æµ‹è¯•é›†: {TEST_START} èµ·

    å‚æ•°æœç´¢:
    - ç²¾é€‰10ä¸ªå‚æ•°ç»„åˆï¼Œè¦†ç›–ä¸åŒé…ç½®
    - åŒ…å«å°å‹ã€ä¸­å‹ã€å¤§å‹ç½‘ç»œ
    - Dropoutç‡: 0.2-0.4
    - å­¦ä¹ ç‡: 0.001, 0.0005

    æœ€ä½³ç»„åˆç»“æœ:
    - éªŒè¯é›†æ”¶ç›Šç‡: {results_df.iloc[0]['valid_return'] * 100:.2f}%
    - æµ‹è¯•é›†æ”¶ç›Šç‡: {test_return * 100:.2f}%
    - æœ€ä½³R2åˆ†æ•°: {results_df.iloc[0]['r2_score']:.4f}

    è¾“å‡ºæ–‡ä»¶:
    1. top10_parameter_results.csv - 10ä¸ªç»„åˆè¯¦ç»†ç»“æœ
    2. best_model_top10_comboX.keras - æœ€ä½³æ¨¡å‹
    3. best_model_params_top10.json - æœ€ä½³æ¨¡å‹å‚æ•°
    4. Top10_Parameter_Report.png - ç»¼åˆæŠ¥å‘Šå›¾è¡¨

    ä½¿ç”¨å»ºè®®:
    1. å¦‚æœæµ‹è¯•é›†è¡¨ç°ä¸ä½³ï¼Œå¯å°è¯•è°ƒæ•´ç­–ç•¥å‚æ•°
    2. æŸ¥çœ‹top10_parameter_results.csvé€‰æ‹©å…¶ä»–æœ‰æ½œåŠ›çš„ç»„åˆ
    3. å¯ä¿®æ”¹SEEDè¿›è¡Œå¤šæ¬¡å®éªŒéªŒè¯ç¨³å®šæ€§
    {'=' * 60}
    """

    print(summary)

    with open('Top10_Validation_Summary.txt', 'w', encoding='utf-8') as f:
        f.write(summary)


if __name__ == '__main__':
    main()
import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score,
    accuracy_score
)
import warnings
warnings.filterwarnings("ignore")

# --- é…ç½® ---
INPUT_FILE = "00700_clean_features.csv"
TRAIN_END = "2023-01-01"
VAL_END = "2024-01-01"

RANDOM_STATE = 42

FEATURES = [
    'Ret_Lag_1', 'Ret_Lag_2', 'Ret_Lag_5',
    'RSI_6', 'RSI_12',
    'MACD_Hist', 'Body_Ratio', 'Bias_20', 'ATR'
]

# ğŸ”§ ä¼˜åŒ–åçš„ XGBoost å‚æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
XGB_PARAMS = {
    'objective': 'multi:softmax',
    'num_class': 3,
    'eta': 0.05,               # å­¦ä¹ ç‡é™ä½
    'max_depth': 3,            # å‡å°æ ‘æ·±åº¦
    'min_child_weight': 5,     # æé«˜æœ€å°æ ·æœ¬æƒé‡
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,         # L2 æ­£åˆ™
    'reg_alpha': 0.1,          # L1 æ­£åˆ™
    'random_state': RANDOM_STATE,
    'verbosity': 0
}
NUM_ROUND = 100  # å‡å°‘è½®æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ


def load_and_split_data():
    df = pd.read_csv(INPUT_FILE, parse_dates=['Date'], index_col='Date')
    df.sort_index(inplace=True)

    train_df = df[df.index < TRAIN_END].copy()
    val_df = df[(df.index >= TRAIN_END) & (df.index < VAL_END)].copy()
    test_df = df[df.index >= VAL_END].copy()

    print(f"ğŸ“Š æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {train_df.index.min().date()} ~ {train_df.index.max().date()} ({len(train_df)} æ¡)")
    print(f"   éªŒè¯é›†: {val_df.index.min().date()} ~ {val_df.index.max().date()} ({len(val_df)} æ¡)")
    print(f"   å›æµ‹é›†: {test_df.index.min().date()} ~ {test_df.index.max().date()} ({len(test_df)} æ¡)")

    return train_df, val_df, test_df


def scale_features(train_df, val_df, test_df):
    scaler = StandardScaler()
    train_df[FEATURES] = scaler.fit_transform(train_df[FEATURES])
    val_df[FEATURES] = scaler.transform(val_df[FEATURES])
    test_df[FEATURES] = scaler.transform(test_df[FEATURES])
    return train_df, val_df, test_df, scaler


def train_model_with_eval(X_train, y_train, X_val, y_val):
    # æ˜ å°„æ ‡ç­¾ï¼š[-1, 0, 1] â†’ [0, 1, 2]
    y_train_mapped = y_train.replace({-1: 0, 0: 1, 1: 2})
    y_val_mapped = y_val.replace({-1: 0, 0: 1, 1: 2})

    model = xgb.XGBClassifier(**XGB_PARAMS, n_estimators=NUM_ROUND)
    
    # âŒ XGBClassifier ä¸æ”¯æŒ eval_set å’Œ early_stopping_rounds
    # åªèƒ½ç”¨åŸç”Ÿ API æ¥åšè®­ç»ƒæ›²çº¿
    model.fit(X_train, y_train_mapped, verbose=False)
    return model


def plot_training_curve_with_manual_tracking(X_train, y_train, X_val, y_val):
    # ä½¿ç”¨åŸç”Ÿ API æ¥ç”»è®­ç»ƒæ›²çº¿
    y_train_mapped = y_train.replace({-1: 0, 0: 1, 1: 2})
    y_val_mapped = y_val.replace({-1: 0, 0: 1, 1: 2})

    dtrain = xgb.DMatrix(X_train, label=y_train_mapped)
    dval = xgb.DMatrix(X_val, label=y_val_mapped)

    evals_result = {}
    bst = xgb.train(
        XGB_PARAMS,
        dtrain,
        num_boost_round=NUM_ROUND,
        evals=[(dtrain, 'train'), (dval, 'validation')],
        evals_result=evals_result,
        verbose_eval=False
    )

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(8, 5))
    plt.plot(evals_result['train']['mlogloss'], label='Train Loss', color='blue')
    plt.plot(evals_result['validation']['mlogloss'], label='Validation Loss', color='red')
    plt.xlabel('Boosting Round')
    plt.ylabel('Multi-class Log Loss')
    plt.title('Training vs Validation Loss Curve')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig('training_curve.png', dpi=150)
    plt.show()

    return bst, evals_result


def evaluate_on_set(model, df, name):
    X = df[FEATURES]
    y_true = df['Target']

    y_pred_mapped = model.predict(X)
    y_pred = pd.Series(y_pred_mapped).replace({0: -1, 1: 0, 2: 1}).values

    macro_f1 = f1_score(y_true, y_pred, average='macro', labels=[-1, 0, 1])
    acc = accuracy_score(y_true, y_pred)

    print(f"\nğŸ“ˆ {name} é›†è¯„ä¼°ç»“æœ:")
    print(f"   Accuracy: {acc:.4f}")
    print(f"   Macro-F1: {macro_f1:.4f}")
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, target_names=['è·Œ (-1)', 'éœ‡è¡ (0)', 'æ¶¨ (+1)'], labels=[-1, 0, 1]))

    return y_true, y_pred


def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])
    plt.figure(figsize=(6, 5))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=['è·Œ', 'éœ‡è¡', 'æ¶¨'],
        yticklabels=['è·Œ', 'éœ‡è¡', 'æ¶¨']
    )
    plt.title(f'{title} - Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(f'confusion_matrix_{title.lower().replace(" ", "_")}.png', dpi=150)
    plt.show()


def backtest_strategy(test_df, y_pred):
    test_df = test_df.copy()
    test_df['Signal'] = y_pred  # [-1, 0, 1]

    # âš ï¸ å…³é”®ï¼šç”¨ä»Šæ—¥ä¿¡å· Ã— æ˜æ—¥æ”¶ç›Šç‡ï¼ˆæ— æœªæ¥ä¿¡æ¯ï¼‰
    test_df['Tomorrow_Return'] = test_df['Close'].shift(-1) / test_df['Close'] - 1

    # åˆ é™¤æœ€åä¸€è¡Œï¼ˆæ— æ³•è®¡ç®—æ˜æ—¥æ”¶ç›Šï¼‰
    test_df = test_df.iloc[:-1].copy()

    # ç­–ç•¥æ”¶ç›Š = ä¿¡å· * æ˜æ—¥æ”¶ç›Šç‡
    test_df['Strategy_Return'] = test_df['Signal'] * test_df['Tomorrow_Return']

    # ç´¯è®¡å‡€å€¼ï¼ˆå¤„ç† NaNï¼‰
    test_df['Strategy_Return'] = test_df['Strategy_Return'].fillna(0)
    test_df['Tomorrow_Return'] = test_df['Tomorrow_Return'].fillna(0)

    test_df['Cumulative_Strategy'] = (1 + test_df['Strategy_Return']).cumprod()
    test_df['Cumulative_BuyHold'] = (1 + test_df['Tomorrow_Return']).cumprod()

    # ç»©æ•ˆæŒ‡æ ‡
    total_trades = (test_df['Signal'] != 0).sum()
    win_trades = ((test_df['Signal'] * test_df['Tomorrow_Return']) > 0).sum()
    win_rate = win_trades / total_trades if total_trades > 0 else 0
    total_profit = test_df['Strategy_Return'].sum()
    sharpe = test_df['Strategy_Return'].mean() / test_df['Strategy_Return'].std() * np.sqrt(252) \
             if test_df['Strategy_Return'].std() != 0 else 0

    print(f"\nğŸ’¼ å›æµ‹ç»©æ•ˆæ‘˜è¦:")
    print(f"   æ€»äº¤æ˜“æ¬¡æ•°: {total_trades}")
    print(f"   èƒœç‡: {win_rate:.2%}")
    print(f"   æ€»æ”¶ç›Š: {total_profit:.2%}")
    print(f"   å¹´åŒ–å¤æ™®æ¯”ç‡: {sharpe:.2f}")

    # å‡€å€¼æ›²çº¿
    plt.figure(figsize=(12, 6))
    plt.plot(test_df.index, test_df['Cumulative_Strategy'], label='ç­–ç•¥å‡€å€¼', linewidth=2)
    plt.plot(test_df.index, test_df['Cumulative_BuyHold'], label='ä¹°å…¥æŒæœ‰', linewidth=1, alpha=0.7)
    plt.title('ğŸ“ˆ å›æµ‹å‡€å€¼æ›²çº¿ (2024å¹´èµ·)')
    plt.xlabel('æ—¥æœŸ')
    plt.ylabel('ç´¯è®¡å‡€å€¼')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig('equity_curve.png', dpi=150)
    plt.show()


def main():
    print("ğŸš€ [Step 1] åŠ è½½å¹¶åˆ’åˆ†æ•°æ®...")
    train_df, val_df, test_df = load_and_split_data()

    print(f"\nğŸ” ä½¿ç”¨ç‰¹å¾åˆ—: {FEATURES}")

    print("\nğŸ”„ [Step 2] ç‰¹å¾æ ‡å‡†åŒ–...")
    train_df, val_df, test_df, scaler = scale_features(train_df, val_df, test_df)

    print("\nğŸ§  [Step 3] è®­ç»ƒ XGBoost æ¨¡å‹...")
    X_train, y_train = train_df[FEATURES], train_df['Target']
    X_val, y_val = val_df[FEATURES], val_df['Target']

    model = train_model_with_eval(X_train, y_train, X_val, y_val)

    print("\nğŸ“‰ [Step 4] ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    _, _ = plot_training_curve_with_manual_tracking(X_train, y_train, X_val, y_val)

    print("\nğŸ“Š [Step 5] æ¨¡å‹è¯„ä¼°...")
    evaluate_on_set(model, train_df, "è®­ç»ƒ")
    evaluate_on_set(model, val_df, "éªŒè¯")
    y_true_test, y_pred_test = evaluate_on_set(model, test_df, "å›æµ‹")

    print("\nğŸ§© [Step 6] æ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix(y_true_test, y_pred_test, "å›æµ‹é›†")

    print("\nğŸ’° [Step 7] ç­–ç•¥å›æµ‹...")
    backtest_strategy(test_df, y_pred_test)

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å›¾è¡¨å·²ä¿å­˜ä¸º PNG æ–‡ä»¶ã€‚")


if __name__ == "__main__":
    main()